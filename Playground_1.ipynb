{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "178fc9aa-e9d6-4887-893e-a51eec8ef3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6893b241-b49f-4e1a-b434-fd07d5229e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9003f11c-ae57-4887-aa9f-394ce0160f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with feature visualization\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        model = models.resnext50_32x4d(pretrained = True)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048,num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    def forward(self, x):\n",
    "        batch_size,seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size,seq_length,2048)\n",
    "        x_lstm,_ = self.lstm(x,None)\n",
    "        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c2a5a12-c20b-449b-a411-54ee5aec9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 112\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "sm = nn.Softmax()\n",
    "inv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.squeeze()\n",
    "    image = inv_normalize(image)\n",
    "    image = image.numpy()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image.clip(0, 1)\n",
    "    cv2.imwrite('./2.png',image*255)\n",
    "    return image\n",
    "\n",
    "def predict(model,img,path = './'):\n",
    "  fmap,logits = model(img.to('cuda'))\n",
    "  params = list(model.parameters())\n",
    "  weight_softmax = model.linear1.weight.detach().cpu().numpy()\n",
    "  logits = sm(logits)\n",
    "  _,prediction = torch.max(logits,1)\n",
    "  confidence = logits[:,int(prediction.item())].item()*100\n",
    "  print('confidence of prediction:',logits[:,int(prediction.item())].item()*100)\n",
    "  idx = np.argmax(logits.detach().cpu().numpy())\n",
    "  bz, nc, h, w = fmap.shape\n",
    "  out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n",
    "  predict = out.reshape(h,w)\n",
    "  predict = predict - np.min(predict)\n",
    "  predict_img = predict / np.max(predict)\n",
    "  predict_img = np.uint8(255*predict_img)\n",
    "  out = cv2.resize(predict_img, (im_size,im_size))\n",
    "  #heatmap = \n",
    "  img = im_convert(img[:,-1,:,:,:])\n",
    "  result =  img*0.8*255\n",
    "  cv2.imwrite('/content/drive/MyDrive/Qriocity/Secure Vision/Subset/Deep-Fake/train/fake_0_jpg.rf.627e22b7f278a118695d126f4ee791f6.jpg',result)\n",
    "  result1 = img*0.8\n",
    "  r,g,b = cv2.split(result1)\n",
    "  result1 = cv2.merge((r,g,b))\n",
    "  plt.imshow(result1)\n",
    "  plt.show()\n",
    "  return [int(prediction.item()),confidence]\n",
    "#img = r\"C:\\Users\\shubh\\Downloads\\003.png\"\n",
    "#predict(model,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475420ba-c1ef-4887-b1a5-1955435f5f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85263717-157e-4fb2-9ab0-642ada46b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install face_recognition\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "class validation_dataset(Dataset):\n",
    "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
    "        self.video_names = video_names\n",
    "        self.transform = transform\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    def __getitem__(self,idx):\n",
    "        video_path = self.video_names[idx]\n",
    "        frames = []\n",
    "        a = int(100/self.count)\n",
    "        first_frame = np.random.randint(0,a) \n",
    "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
    "          frames.append(self.transform(frame))\n",
    "          if(len(frames) == self.count):\n",
    "            break\n",
    "        frames = torch.stack(frames)\n",
    "        frames = frames[:self.count]\n",
    "        return frames.unsqueeze(0)\n",
    "    def frame_extract(self,path):\n",
    "      vidObj = cv2.VideoCapture(path) \n",
    "      success = 1\n",
    "      while success:\n",
    "          success, image = vidObj.read()\n",
    "          if success:\n",
    "              yield image\n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1,2,0)\n",
    "    b,g,r = cv2.split(image)\n",
    "    image = cv2.merge((r,g,b))\n",
    "    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n",
    "    image = image*255.0\n",
    "    plt.imshow(image.astype(int))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8781410a-b769-4dba-9b0b-3e72c4f2e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\Downloads\\abarnvbtwb.mp4\n",
      "confidence of prediction: 84.69574451446533\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL\n"
     ]
    }
   ],
   "source": [
    "#Code for making prediction\n",
    "im_size = 112\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.ToPILImage(),\n",
    "                                        transforms.Resize((im_size,im_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean,std)])\n",
    "#path_to_videos = ['/content/drive/My Drive/Balanced_Face_only_data/aagfhgtpmv.mp4',\n",
    "                                  # '/content/drive/My Drive/Balanced_Face_only_data/aczrgyricp.mp4',\n",
    "                                  # '/content/drive/My Drive/Balanced_Face_only_data/agdkmztvby.mp4',\n",
    "                                  # '/content/drive/My Drive/Balanced_Face_only_data/abarnvbtwb.mp4']\n",
    "\n",
    "#path_to_videos = ['/content/drive/My Drive/Youtube_Face_only_data/000_003.mp4',\n",
    "                 # '/content/drive/My Drive/Youtube_Face_only_data/000.mp4',\n",
    "                 # '/content/drive/My Drive/Youtube_Face_only_data/002_006.mp4',\n",
    "                 # '/content/drive/My Drive/Youtube_Face_only_data/002.mp4'\n",
    "                  \n",
    "\n",
    "#]\n",
    "\n",
    "path_to_videos= [r\"C:\\Users\\shubh\\Downloads\\abarnvbtwb.mp4\"]\n",
    "\n",
    "video_dataset = validation_dataset(path_to_videos,sequence_length = 20,transform = train_transforms)\n",
    "model =Model(2).cuda()\n",
    "path_to_model = r\"C:\\Users\\shubh\\Downloads\\model_97_acc_100_frames_FF_data.pt\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "for i in range(0,len(path_to_videos)):\n",
    "  print(path_to_videos[i])\n",
    "  prediction = predict(model,video_dataset[i],'./')\n",
    "  if prediction[0] == 1:\n",
    "    print(\"REAL\")\n",
    "  else:\n",
    "    print(\"FAKE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020c8b71-e40e-4079-a89f-ce4c7d9d6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional : If you want to pass full frame for prediction instead of face cropped frame\n",
    "#code for full frame processing\n",
    "class validation_dataset(Dataset):\n",
    "    def __init__(self,video_names,sequence_length = 60,transform = None):\n",
    "        self.video_names = video_names\n",
    "        self.transform = transform\n",
    "        self.count = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)\n",
    "    def __getitem__(self,idx):\n",
    "        video_path = self.video_names[idx]\n",
    "        frames = []\n",
    "        a = int(100/self.count)\n",
    "        first_frame = np.random.randint(0,a) \n",
    "        for i,frame in enumerate(self.frame_extract(video_path)):\n",
    "          frames.append(self.transform(frame))\n",
    "          if(len(frames) == self.count):\n",
    "            break\n",
    "        frames = torch.stack(frames)\n",
    "        frames = frames[:self.count]\n",
    "        return frames.unsqueeze(0)\n",
    "    def frame_extract(self,path):\n",
    "      vidObj = cv2.VideoCapture(path) \n",
    "      success = 1\n",
    "      while success:\n",
    "          success, image = vidObj.read()\n",
    "          if success:\n",
    "              yield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b135f222-326f-4fe8-8b31-64c165f0c0e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Use torch.cuda.empty_cache() to release all unoccupied cached memory currently held by the caching allocator.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\cuda\\memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming 'tensor' is the tensor you want to delete\n",
    "\n",
    "\n",
    "# Assuming 'model' is the model you want to delete\n",
    "del model\n",
    "\n",
    "# Use torch.cuda.empty_cache() to release all unoccupied cached memory currently held by the caching allocator.\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f17f449-4366-46ae-abe8-7ce9b97919e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.67 GiB is allocated by PyTorch, and 97.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m video_dataset \u001b[38;5;241m=\u001b[39m VideoDataset(path_to_videos, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, transform\u001b[38;5;241m=\u001b[39mvideo_transforms)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming the model architecture is defined and loaded properly\u001b[39;00m\n\u001b[0;32m     61\u001b[0m path_to_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshubh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_97_acc_100_frames_FF_data.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(path_to_model))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\nn\\modules\\module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:216\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 216\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\nn\\modules\\module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.67 GiB is allocated by PyTorch, and 97.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "#from model import Model  # Assuming you have defined the model architecture in a separate file\n",
    "\n",
    "# Define the dataset class to load and preprocess video frames\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, sequence_length, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames)\n",
    "        if len(frames) < self.sequence_length:\n",
    "            frames = torch.cat([frames] * (self.sequence_length // len(frames) + 1))\n",
    "        frames = frames[:self.sequence_length]\n",
    "        return frames\n",
    "\n",
    "# Define image transformation\n",
    "# Define image transformation\n",
    "im_size = 112\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Define paths to video files\n",
    "path_to_videos = [r\"C:\\Users\\shubh\\Downloads\\Untitled video - Made with Clipchamp (9).mp4\"]\n",
    "\n",
    "# Initialize the dataset\n",
    "video_dataset = VideoDataset(path_to_videos, sequence_length=20, transform=video_transforms)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = Model(2).cuda()  # Assuming the model architecture is defined and loaded properly\n",
    "path_to_model = r\"C:\\Users\\shubh\\Downloads\\model_97_acc_100_frames_FF_data.pt\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Make predictions on each video\n",
    "# Make predictions on each video\n",
    "for video_path in path_to_videos:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames in the video\n",
    "    \n",
    "    # Define the sequence length (you can choose a fixed length or use the total number of frames)\n",
    "    sequence_length = frame_count  # Use total number of frames as sequence length\n",
    "    \n",
    "    for frame_num in range(3):  # Display predictions for only the first three frames\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        frame_tensor = video_transforms(frame).unsqueeze(0).unsqueeze(0).cuda()  # Add batch and sequence dimension\n",
    "        \n",
    "        # Repeat the frame along the sequence dimension\n",
    "        frames = frame_tensor.repeat(1, sequence_length, 1, 1, 1)  # Repeat the frame along the sequence dimension\n",
    "        \n",
    "        prediction = predict(model, frames)\n",
    "        print(f\"Frame {frame_num}: {prediction}\")\n",
    "    cap.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8590ecb1-aa26-4333-9abc-583d542adb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_NO_CUDA_MEMORY_CACHING=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f8ba82a-4c7f-44bf-8038-3ab7e7e3e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec061b4-b498-42b2-9ae2-e35c43cbbf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90a42f0-62a2-4e7e-b2b9-7303b168ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\anaconda3\\envs\\ns\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shubh\\anaconda3\\envs\\ns\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     79\u001b[0m path_to_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshubh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_97_acc_100_frames_FF_data.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 80\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     81\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Define softmax layer and inverse normalization\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ns\\Lib\\site-packages\\torch\\serialization.py:1276\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m deserialized_objects\n\u001b[0;32m   1275\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1276\u001b[0m \u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_should_read_directly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the dataset class to load and preprocess video frames\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, sequence_length, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames)\n",
    "        if len(frames) < self.sequence_length:\n",
    "            frames = torch.cat([frames] * (self.sequence_length // len(frames) + 1))\n",
    "        frames = frames[:self.sequence_length]\n",
    "        return frames\n",
    "\n",
    "# Define image transformation\n",
    "im_size = 112\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Convert color channels from [0, 255] to [0, 1]\n",
    "def convert_color_channels(frame):\n",
    "    return frame / 255.0\n",
    "\n",
    "# Define paths to video files\n",
    "path_to_videos = [r\"C:\\Users\\shubh\\Downloads\\abarnvbtwb.mp4\"]\n",
    "\n",
    "# Initialize the dataset\n",
    "video_dataset = VideoDataset(path_to_videos, sequence_length=20, transform=video_transforms)\n",
    "\n",
    "# Load the pre-trained model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(*list(models.resnext50_32x4d(pretrained=True).children())[:-2])\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(2048, num_classes)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        fmap = self.model(x)\n",
    "        x = self.avgpool(fmap)\n",
    "        x = x.view(batch_size, seq_length, 2048)\n",
    "        x_lstm, _ = self.lstm(x, None)\n",
    "        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))\n",
    "\n",
    "model = Model(2).cuda()\n",
    "path_to_model = r\"C:\\Users\\shubh\\Downloads\\model_97_acc_100_frames_FF_data.pt\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "\n",
    "# Define softmax layer and inverse normalization\n",
    "sm = nn.Softmax()\n",
    "inv_normalize = transforms.Normalize(mean=-1*np.divide(mean, std), std=np.divide([1, 1, 1], std))\n",
    "\n",
    "# Function to convert tensor to image\n",
    "def im_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.squeeze()\n",
    "    image = inv_normalize(image)\n",
    "    image = image.numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(model, img, path='./'):\n",
    "    fmap, logits = model(img.to('cuda'))\n",
    "    logits = sm(logits)\n",
    "    _, prediction = torch.max(logits, 1)\n",
    "    confidence = logits[:, int(prediction.item())].item() * 100\n",
    "\n",
    "    # Further processing for visualization\n",
    "    img = im_convert(img[:, -1, :, :, :])\n",
    "    heatmap = generate_heatmap(fmap, model.linear1.weight.detach().cpu().numpy())\n",
    "    result = blend_heatmap_with_image(heatmap, img)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite('result.jpg', result)\n",
    "\n",
    "    # Display the result\n",
    "    plt.imshow(result)\n",
    "    plt.show()\n",
    "\n",
    "    return [int(prediction.item()), confidence]\n",
    "\n",
    "def generate_heatmap(fmap, weight_softmax):\n",
    "    bz, nc, h, w = fmap.shape\n",
    "    print(\"Shape of fmap:\", fmap.shape)\n",
    "    print(\"Shape of weight_softmax:\", weight_softmax.shape)\n",
    "    out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T, weight_softmax.T)\n",
    "    print(\"Shape of out:\", out.shape)\n",
    "    predict = out.reshape(h, w)\n",
    "    print(\"Shape of predict:\", predict.shape)\n",
    "    predict = predict - np.min(predict)\n",
    "    predict_img = predict / np.max(predict)\n",
    "    predict_img = np.uint8(255 * predict_img)\n",
    "    return predict_img\n",
    "\n",
    "\n",
    "# Function to blend heatmap with original image\n",
    "def blend_heatmap_with_image(heatmap, img):\n",
    "    result = heatmap * 0.5 + img * 0.8 * 255\n",
    "    result = np.clip(result, 0, 255)\n",
    "    result = result.astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "# Make predictions on each video\n",
    "for video_path in path_to_videos:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get total number of frames in the video\n",
    "\n",
    "    # Define the sequence length (you can choose a fixed length or use the total number of frames)\n",
    "    sequence_length = frame_count  # Use total number of frames as sequence length\n",
    "\n",
    "    for frame_num in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        frame = convert_color_channels(frame)  # Convert color channels to [0, 1]\n",
    "        frame_tensor = video_transforms(frame).unsqueeze(0).unsqueeze(0).cuda() \n",
    "        frames = frame_tensor.repeat(1, sequence_length, 1, 1, 1)\n",
    "        prediction = predict(model, frames)\n",
    "        print(f\"Frame {frame_num}: {prediction}\")\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "882c7adf-0e33-4510-8946-b4788ac618c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have defined the Model class in a separate file\n",
    "# from model import Model\n",
    "\n",
    "# Define the dataset class to load and preprocess video frames\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, transform=None):\n",
    "        self.video_paths = video_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "# Define image transformation\n",
    "im_size = 112\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "video_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((im_size, im_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = Model(2).cuda()  # Assuming the model architecture is defined and loaded properly\n",
    "path_to_model = r\"C:\\Users\\shubh\\Downloads\\model_97_acc_100_frames_FF_data.pt\"\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "\n",
    "# Define function to predict and overlay result on frames\n",
    "sm = nn.Softmax()\n",
    "inv_normalize = transforms.Normalize(mean=-1*np.divide(mean,std), std=np.divide([1,1,1], std))\n",
    "def predict_and_overlay(model, frames, path='./output.mp4'):\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Define the codec\n",
    "    out = cv2.VideoWriter(path, fourcc, 25, (width, height))  # Create VideoWriter object\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frame in frames:\n",
    "            frame_tensor = video_transforms(frame).unsqueeze(0).unsqueeze(0).cuda()\n",
    "            fmap, logits = model(frame_tensor)\n",
    "            prediction, confidence = predict(model, frame_tensor)\n",
    "            overlay_text = f\"Prediction: {prediction}, Confidence: {confidence:.2f}%\"\n",
    "            \n",
    "            # Convert RGB to BGR\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Draw overlay text\n",
    "            cv2.putText(frame_bgr, overlay_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Write frame with overlay text to video\n",
    "            out.write(frame_bgr)\n",
    "    \n",
    "    out.release()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, img):\n",
    "    fmap, logits = model(img)\n",
    "    logits = sm(logits)\n",
    "    _, prediction = torch.max(logits, 1)\n",
    "    confidence = logits[:, int(prediction.item())].item() * 100\n",
    "    return int(prediction.item()), confidence\n",
    "\n",
    "# Define paths to video files\n",
    "path_to_videos = [r\"C:\\Users\\shubh\\Downloads\\download.mp4\"]\n",
    "\n",
    "# Initialize the dataset\n",
    "video_dataset = VideoDataset(path_to_videos, transform=None)\n",
    "\n",
    "# Iterate over each video in the dataset\n",
    "for video_frames in video_dataset:\n",
    "    predict_and_overlay(model, video_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7745487-c71b-4847-a9f6-25ed6e38b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
